{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1f8c001-d22e-4682-a0d3-e6386c9d648d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('spark_streaming').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fded61c8-90cd-4c3f-a861-5d6e865bff9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = spark.read.format('csv').option('header', True).load('/Volumes/workspace/default/spark_data/emp.csv')\n",
    "emp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c359e62-7a74-4c76-9156-c9b2614474db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp.write.mode('append').format('delta').saveAsTable('workspace.default.emp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d60eadc-b47e-4e80-85ce-b34927bf7c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ---->read and write streamig data from table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96dc51e6-a271-4fcd-adae-1e1ae4c724e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.option('header', True).table('workspace.default.emp').display(checkpointLocation = '/Volumes/workspace/default/spark_data/checkpoint') # create a checkpoint file\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08519f71-c5bf-4ec1-9bd9-f5cf7a43bd9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS stm_emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad29e957-acf9-44f3-a875-deac44c3e78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.option('header',True).table('workspace.default.emp')\n",
    "\n",
    "df.writeStream.format('delta')\\\n",
    "    .option('checkpointLocation', '/Volumes/workspace/default/spark_data/checkpoint/')\\\n",
    "        .trigger(availableNow = True)\\\n",
    "            .toTable('stm_emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247fcc24-cfdd-4b6e-8b2d-3b3a717e730b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.option('header',True).table('workspace.default.emp')\n",
    "\n",
    "df.writeStream.format('delta')\\\n",
    "    .option('overwriteSchema',True)\\\n",
    "        .outputMode('append')\\\n",
    "            .option('checkpointLocation','/Volumes/workspace/default/spark_data/checkpoint1/')\\\n",
    "                .trigger(availableNow = True)\\\n",
    "                .toTable('cbcatalog.default.stm_emp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec8d012-1818-4502-b1b4-c94bf512f567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from stm_emp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f109689a-94fa-4d73-9912-e9a14282220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ---->read and write streamig data from volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9897fb9f-6ef6-4962-953f-57cf0342915e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField('empno',IntegerType(),False),\n",
    "                     StructField('ename',StringType(),True),\n",
    "                     StructField('job',StringType(),True),\n",
    "                     StructField('mgr',IntegerType(),True),\n",
    "                     StructField('hiredate',StringType(),True),\n",
    "                     StructField('sal',IntegerType(),True),\n",
    "                     StructField('comm',IntegerType(),True),\n",
    "                     StructField('deptno',IntegerType(),True) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75199fa7-4b7f-4a6c-a398-63c830fc70eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream\\\n",
    "    .format('csv')\\\n",
    "        .schema(schema)\\\n",
    "        .option('header',True)\\\n",
    "            .load('/Volumes/cbcatalog/cbschema/volume2/streaming_fol/')\n",
    "\n",
    "df.writeStream\\\n",
    "    .format('delta')            \\\n",
    "        .option('overwriteSchema',True)\\\n",
    "            .outputMode('append')\\\n",
    "                .trigger(availableNow = True)\\\n",
    "                .option('checkpointLocation','/Volumes/cbcatalog/cbschema/volume2/checkpoint')\\\n",
    "                    .toTable('cbcatalog.default.stm_tab1') # option('path', volumme path)  for volume store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84cf41a0-f144-448f-9767-945097ccd475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from cbcatalog.default.stm_tab1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317ff311-9d71-4b93-84a6-0af56c65cebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### -->Modes in spark Streaming\n",
    "pyspark has a method outputMood() to specify the saving mode\n",
    "## 1.Complete\n",
    "in complite mood the hole data of source file will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "566bbe4e-6a0f-4933-bfc6-deab237c7247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_stream = spark.readStream\\\n",
    "    .format('cloudFiles')\\\n",
    "    .option('cloudFiles.format', 'CSV')\\\n",
    "    .option('header', True)\\\n",
    "    .option('cloudFiles.schemaLocation', '/Volumes/workspace/default/spark_data/schemaLocation')\\\n",
    "    .load('/Volumes/workspace/default/spark_data/streaming_data/') # create a directory to read increamental data coming from multiple sources\n",
    "\n",
    "result = emp_stream.writeStream\\\n",
    "    .format('delta')\\\n",
    "    .outputMode('complete')\\      # append mood or coplite mood\n",
    "    .option('mergeSchema', True)\\\n",
    "    .option('checkpointLocation', '/Volumes/workspace/default/spark_data/checkpoint/')\\\n",
    "    .trigger(availableNow = True)\\      # if the data is available then it process it otherwise not\n",
    "    .toTable('streaming_emp') # store data as a delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a8397dc-3140-4c17-b3d7-8bafbbee1e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa41682-d5f6-467c-b3f5-32952b60a8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_stream = spark.readStream\\\n",
    "    .format('cloudFiles')\\\n",
    "    .option('cloudFiles.format', 'CSV')\\\n",
    "    .option('header', True)\\\n",
    "    .option('cloudFiles.schemaLocation', '/Volumes/workspace/default/spark_data/schemaLocation')\\\n",
    "    .load('/Volumes/workspace/default/spark_data/streaming_data/') # create a directory to read increamental data coming from multiple sources\n",
    "\n",
    "result = emp_stream.writeStream\\\n",
    "    .format('delta')\\\n",
    "    .outputMode('append')\\    # in append mood only the increamental data will be store(previous data + new data)\n",
    "    .option('mergeSchema', True)\\\n",
    "    .option('checkpointLocation', '/Volumes/workspace/default/spark_data/checkpoint/')\\\n",
    "    .trigger(availableNow = True)\\      # if the data is available then it process it otherwise not\n",
    "    .toTable('streaming_emp') # store data as a delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfc8bf4f-70d6-4505-bb63-5ffcbe491673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9560eac8-12da-4829-bc6d-9b36b7be30bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_stream = spark.readStream\\\n",
    "    .format('cloudFiles')\\\n",
    "    .option('cloudFiles.format', 'CSV')\\\n",
    "    .option('header', True)\\\n",
    "    .option('cloudFiles.schemaLocation', '/Volumes/workspace/default/spark_data/schemaLocation')\\\n",
    "    .load('/Volumes/workspace/default/spark_data/streaming_data/') # create a directory to read increamental data coming from multiple sources\n",
    "\n",
    "result = emp_stream.writeStream\\\n",
    "    .format('delta')\\\n",
    "    .outputMode('update')\\      # append mood or coplite mood\n",
    "    .option('mergeSchema', True)\\\n",
    "    .option('checkpointLocation', '/Volumes/workspace/default/spark_data/checkpoint/')\\\n",
    "    .trigger(availableNow = True)\\      # if the data is available then it process it otherwise not\n",
    "    .toTable('streaming_emp') # store data as a delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebaaefe8-c830-4cfd-b2f8-e4653965b9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### json transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b3c127-f3cb-4019-b18f-090ffad1b97e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"address\":596},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760157745088}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust = spark.read.format('json').option('multiLine',True).load('/Volumes/workspace/default/spark_data/cust_json.json')\n",
    "cust.display()\n",
    "# create a schema explicitly if you want but do not use inferSchema bcz it does not allow to select the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48b21a1f-bce5-4a85-bf18-aa64ebdb8c78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "72f2ca2a-3b23-4791-997b-23c08efd0783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer\", StructType([\n",
    "        StructField(\"customerId\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"phone\", StringType(), True),\n",
    "        StructField(\"address\", StructType([\n",
    "            StructField(\"street\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "            StructField(\"postalCode\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"order\", StructType([\n",
    "        StructField(\"orderId\", StringType(), True),\n",
    "        StructField(\"orderDate\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"items\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"productId\", StringType(), True),\n",
    "                StructField(\"productName\", StringType(), True),\n",
    "                StructField(\"quantity\", IntegerType(), True),\n",
    "                StructField(\"price\", DoubleType(), True),\n",
    "                StructField(\"supplier\", StructType([\n",
    "                    StructField(\"name\", StringType(), True),\n",
    "                    StructField(\"contact\", StringType(), True)\n",
    "                ]), True)\n",
    "            ])\n",
    "        ), True),\n",
    "        StructField(\"totalAmount\", DoubleType(), True),\n",
    "        StructField(\"paymentMethod\", StringType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c111c45-317d-43e4-9307-7920a10e0afe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema is defined above cell\n",
    "cust = spark.read.format('json').option('multiline',True).schema(schema).load('/Volumes/workspace/default/spark_data/simp_json2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf474c8-ad16-4ade-a3b4-f8564dd301dd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"items\":636},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760185586824}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the items column\n",
    "cust = cust.select(col('customer.customerId').alias('cust_id'), col('customer.name').alias('name'),col('order.items').alias('items'), col('order.orderDate').alias('ord_dt'), col('order.paymentMethod').alias('pay_mthd'))\n",
    "display(cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd54002a-8d85-4e43-980c-4416d5d2294e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cust1 = cust.withColumn('xpld_itm', explode_outer(col('items')))\n",
    "cust1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4506fa17-53dd-4850-af4c-c710e7cf429e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"p_id\":119,\"p_name\":227},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760188209501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = cust1.select('cust_id', 'name', col('items.productId').alias('p_id'), col('items.productName').alias('p_name'), col('items.quantity').alias('qty'), col('items.price').alias('price'), 'ord_dt', 'pay_mthd' )\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e4839e-feda-428b-93ab-dcb9535ff28b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"p_id\":118},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760189806387}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('id_nm', arrays_zip(col('p_id'), col('p_name')) ).withColumn('qt_pc', arrays_zip(col('qty'),col('price')))\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88061a0-35cb-440e-9baa-08102cf18125",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"map_prod\":306,\"map_val\":163},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760240425502}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.withColumn('map_prod', map_from_entries(col('id_nm'))).withColumn('map_val', map_from_entries(col('qt_pc')))\n",
    "df1.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ac2e34b-13eb-42ce-8a0f-a9b101a1ec67",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760240878101}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df1.select('cust_id','name',explode(col('map_prod')).alias('p_id', 'p_name'),explode(col('map_val')).alias('qty', 'price'), 'ord_dt', 'pay_mthd').dropDuplicates()\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21515757-f626-483b-be3f-5b6d42c4bea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### streaming in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f664d9-3e7e-4130-8259-6a3c588a39df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# first we have to save the transformed json file as a delta table bcz spark does not allows to write a normal dataFrame as  streaming dataFrame\n",
    "df_final.write.format('delta').option('header',True).option('overwriteSchema', 'true').saveAsTable('smpl_stm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c27637-1a12-485c-9be4-1c21875ab357",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# we can read a normal delta table as a streaming dataFrame\n",
    "df = spark.readStream.format('delta')\\\n",
    "    .option('header',True)\\\n",
    "        .load('workspace.default.smpl_stm')\n",
    "\n",
    "df.writeStream.format('delta')\\\n",
    "    .outputMode('append')\\\n",
    "    .option('checkpointLocation','/Volumes/cbcatalog/cbschema/volume2/checkpoint1')\\\n",
    "        .option('mergeSchema',True)\\\n",
    "            .trigger(availableNow = True)\\\n",
    "            .option('path', '/Volumes/cbcatalog/cbschema/volume2/smpl_stm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18d889c-4290-4ab9-a3c0-21f4c9bc3987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table workspace.default.stm_colr (color_nm string,timing timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89262046-9a88-4c84-b6c5-132766a0a628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into workspace.default.stm_colr values ('red','2025-1-10T10:05:00'), ('green','2025-2-10T10:10:00'), ('blue','2025-3-10T10:15:00'), ('yellow','2025-4-10T10:20:00'), ('orange','2025-5-10T10:25:00'), ('orange','2025-5-10T10:25:00'), ('red','2025-1-10T10:05:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ed98f5a-b92c-4866-9f87-a25868d1f625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.default. stm_colr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26079e0a-b315-4a92-a906-8468d7c630e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_colr = spark.readStream.format('delta')\\\n",
    "    .option('header',True)\\\n",
    "        .load('workspace.default.stm_colr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b08518a2-ffe9-4817-a601-19446c69b030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cnt_colr = spark.sql('select color_nm, count(*) from workspace.default.stm_colr group by color_nm')\n",
    "cnt_colr.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37653061-e563-4607-8710-db403f9e8ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_colr.writeStream.format('delta')\\\n",
    "    .option('header',True)\\\n",
    "        .outputMode('append')\\\n",
    "            .option('append',True)\\\n",
    "                .option('checkpointLocation','dbfs:/Volumes/workspace/default/spark_data/checkpoint_colr')\\\n",
    "                    .trigger(availableNow = True)\\\n",
    "                        .option('path','/Volumes/workspace/default/spark_data.stm_colr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5e48da5-73ef-431b-afda-dceaf0ebe780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_colr = spark.readStream.format('delta')\\\n",
    "    .option('header', True)\\\n",
    "    .load('workspace.default.stm_colr')\n",
    "\n",
    "cnt_colr = df_colr.groupBy('color_nm').count()\n",
    "display(cnt_colr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c62b2e-1563-452b-9a3e-e2285d9312e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You must call .start() to begin the streaming write and actually store data at the specified location.\n",
    "df_colr.writeStream.format('delta')\\\n",
    "    .option('header', True)\\\n",
    "    .outputMode('overwrite')\\\n",
    "    .option('mergeSchema', True)\\\n",
    "    .option('checkpointLocation', '/Volumes/cbcatalog/cbschema/volume2/checkpoint2/')\\\n",
    "    .trigger(availableNow=True)\\\n",
    "    .option('path', 'dbfs:/Volumes/cbcatalog/cbschema/volume2/cnt_colr')\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66afbefb-d7b4-470a-994e-342ae89f672f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a68eabaa-899f-4042-941c-efd871bb2d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ---->window operation in Streaming\n",
    "## 1.Tumbling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b1b171-9db5-46a4-b41d-be5d63f0cdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Example of applying tumbling window in Apache Spark\n",
    "df = spark.readStream.format('csv').option('header', True).load('/path/to/data')\n",
    "\n",
    "tumbling_window_df = df.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"5 minutes\"), \"id\") \\\n",
    "    .count()\n",
    "\n",
    "query = tumbling_window_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "782a1f32-795a-4ca5-b094-f20232b535ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5918bafa-29cd-4d53-ad1c-7117de1763c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b20e256-31ed-46bd-ba07-2ad8313ba6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.session window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "144b8cd6-dbce-402f-8aaa-2d4933fe9cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27997511-83d0-4c3b-901d-c7a647056aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WaterMarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72891efd-c31b-4edd-b1ce-03149d81b9dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Example of applying watermarking in Apache Spark\n",
    "df = spark.readStream.format('csv').option('header', True).load('/path/to/data')\n",
    "\n",
    "watermarked_df = df.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(window(\"event_time\", \"5 minutes\"), \"id\") \\\n",
    "    .count()\n",
    "\n",
    "query = watermarked_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc43d0e8-19a2-4b13-a931-8e81085c0208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_stream = spark.readStream\\\n",
    "    .format('cloudFiles')\\\n",
    "    .option('cloudFiles.format', 'CSV')\\\n",
    "    .option('header', True)\\\n",
    "    .option('cloudFiles.schemaLocation', '/Volumes/workspace/default/spark_data/schemaLocation')\\\n",
    "    .load('/Volumes/workspace/default/spark_data/streaming_data/') # create a directory to read increamental data coming from multiple sources\n",
    "\n",
    "result = emp_stream.writeStream\\\n",
    "    .format('delta')\\\n",
    "    .outputMode('append')\\\n",
    "    .option('mergeSchema', True)\\\n",
    "    .option('checkpointLocation', '/Volumes/workspace/default/spark_data/checkpoint/')\\\n",
    "    .trigger(availableNow = True)\\\n",
    "    .toTable('streaming_emp') # store data as a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d0d98ee-1b81-4ac5-9ccc-e97f2dce4697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from streaming_emp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09ff8868-da39-449b-8977-83fd0e15e937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ---->DLT(Delta Live Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44871644-91a3-49a3-9a2d-9cf9cf38262b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1827280-c894-49e8-b9f2-231fca0ce0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8700044853532333,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_Streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
